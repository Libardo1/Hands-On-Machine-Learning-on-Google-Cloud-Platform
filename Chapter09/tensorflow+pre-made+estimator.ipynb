{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets('data/MNIST/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.test.cls = np.argmax(data.test.labels, axis=1)\n",
    "data.train.cls = np.argmax(data.train.labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of colour channels for the images: 1 channel for gray-scale.\n",
    "num_channels = 1\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(data.train.images)},\n",
    "    y=np.array(data.train.cls),\n",
    "    num_epochs=None,\n",
    "  \n",
    "    batch_size=1024,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.estimator.inputs.numpy_io.input_fn>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'x': <tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(1024, 784) dtype=float32>},\n",
       " <tf.Tensor 'random_shuffle_queue_DequeueMany:2' shape=(1024,) dtype=int64>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(data.test.images)},\n",
    "    y=np.array(data.test.cls),\n",
    "    num_epochs=1,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "some_images = data.test.images[0:9]\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": some_images},\n",
    "    num_epochs=1,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_x = tf.feature_column.numeric_column(\"x\", shape=(784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_NumericColumn(key='x', shape=(784,), default_value=None, dtype=tf.float32, normalizer_fn=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_columns = [feature_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_NumericColumn(key='x', shape=(784,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_hidden_units = [512, 256, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3c01084a90>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/content/datalab/docs/log4', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                   hidden_units=num_hidden_units,\n",
    "                                   activation_fn=tf.nn.relu,\n",
    "                                   n_classes=num_classes,\n",
    "                                  model_dir='/content/datalab/docs/log4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DNNClassifier in module tensorflow.python.estimator.canned.dnn:\n",
      "\n",
      "class DNNClassifier(tensorflow.python.estimator.estimator.Estimator)\n",
      " |  A classifier for TensorFlow DNN models.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  categorical_feature_a = categorical_column_with_hash_bucket(...)\n",
      " |  categorical_feature_b = categorical_column_with_hash_bucket(...)\n",
      " |  \n",
      " |  categorical_feature_a_emb = embedding_column(\n",
      " |      categorical_column=categorical_feature_a, ...)\n",
      " |  categorical_feature_b_emb = embedding_column(\n",
      " |      categorical_column=categorical_feature_b, ...)\n",
      " |  \n",
      " |  estimator = DNNClassifier(\n",
      " |      feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n",
      " |      hidden_units=[1024, 512, 256])\n",
      " |  \n",
      " |  # Or estimator using the ProximalAdagradOptimizer optimizer with\n",
      " |  # regularization.\n",
      " |  estimator = DNNClassifier(\n",
      " |      feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],\n",
      " |      hidden_units=[1024, 512, 256],\n",
      " |      optimizer=tf.train.ProximalAdagradOptimizer(\n",
      " |        learning_rate=0.1,\n",
      " |        l1_regularization_strength=0.001\n",
      " |      ))\n",
      " |  \n",
      " |  # Input builders\n",
      " |  def input_fn_train: # returns x, y\n",
      " |    pass\n",
      " |  estimator.train(input_fn=input_fn_train, steps=100)\n",
      " |  \n",
      " |  def input_fn_eval: # returns x, y\n",
      " |    pass\n",
      " |  metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)\n",
      " |  def input_fn_predict: # returns x, None\n",
      " |    pass\n",
      " |  predictions = estimator.predict(input_fn=input_fn_predict)\n",
      " |  ```\n",
      " |  \n",
      " |  Input of `train` and `evaluate` should have following features,\n",
      " |  otherwise there will be a `KeyError`:\n",
      " |  \n",
      " |  * if `weight_column` is not `None`, a feature with\n",
      " |    `key=weight_column` whose value is a `Tensor`.\n",
      " |  * for each `column` in `feature_columns`:\n",
      " |    - if `column` is a `_CategoricalColumn`, a feature with `key=column.name`\n",
      " |      whose `value` is a `SparseTensor`.\n",
      " |    - if `column` is a `_WeightedCategoricalColumn`, two features: the first\n",
      " |      with `key` the id column name, the second with `key` the weight column\n",
      " |      name. Both features' `value` must be a `SparseTensor`.\n",
      " |    - if `column` is a `_DenseColumn`, a feature with `key=column.name`\n",
      " |      whose `value` is a `Tensor`.\n",
      " |  \n",
      " |  Loss is calculated by using softmax cross entropy.\n",
      " |  \n",
      " |  @compatibility(eager)\n",
      " |  Estimators are not compatible with eager execution.\n",
      " |  @end_compatibility\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DNNClassifier\n",
      " |      tensorflow.python.estimator.estimator.Estimator\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_units, feature_columns, model_dir=None, n_classes=2, weight_column=None, label_vocabulary=None, optimizer='Adagrad', activation_fn=<function relu>, dropout=None, input_layer_partitioner=None, config=None)\n",
      " |      Initializes a `DNNClassifier` instance.\n",
      " |      \n",
      " |      Args:\n",
      " |        hidden_units: Iterable of number hidden units per layer. All layers are\n",
      " |          fully connected. Ex. `[64, 32]` means first layer has 64 nodes and\n",
      " |          second one has 32.\n",
      " |        feature_columns: An iterable containing all the feature columns used by\n",
      " |          the model. All items in the set should be instances of classes derived\n",
      " |          from `_FeatureColumn`.\n",
      " |        model_dir: Directory to save model parameters, graph and etc. This can\n",
      " |          also be used to load checkpoints from the directory into a estimator to\n",
      " |          continue training a previously saved model.\n",
      " |        n_classes: Number of label classes. Defaults to 2, namely binary\n",
      " |          classification. Must be > 1.\n",
      " |        weight_column: A string or a `_NumericColumn` created by\n",
      " |          `tf.feature_column.numeric_column` defining feature column representing\n",
      " |          weights. It is used to down weight or boost examples during training. It\n",
      " |          will be multiplied by the loss of the example. If it is a string, it is\n",
      " |          used as a key to fetch weight tensor from the `features`. If it is a\n",
      " |          `_NumericColumn`, raw tensor is fetched by key `weight_column.key`,\n",
      " |          then weight_column.normalizer_fn is applied on it to get weight tensor.\n",
      " |        label_vocabulary: A list of strings represents possible label values. If\n",
      " |          given, labels must be string type and have any value in\n",
      " |          `label_vocabulary`. If it is not given, that means labels are\n",
      " |          already encoded as integer or float within [0, 1] for `n_classes=2` and\n",
      " |          encoded as integer values in {0, 1,..., n_classes-1} for `n_classes`>2 .\n",
      " |          Also there will be errors if vocabulary is not provided and labels are\n",
      " |          string.\n",
      " |        optimizer: An instance of `tf.Optimizer` used to train the model. Defaults\n",
      " |          to Adagrad optimizer.\n",
      " |        activation_fn: Activation function applied to each layer. If `None`, will\n",
      " |          use `tf.nn.relu`.\n",
      " |        dropout: When not `None`, the probability we will drop out a given\n",
      " |          coordinate.\n",
      " |        input_layer_partitioner: Optional. Partitioner for input layer. Defaults\n",
      " |          to `min_max_variable_partitioner` with `min_slice_size` 64 << 20.\n",
      " |        config: `RunConfig` object to configure the runtime settings.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.estimator.estimator.Estimator:\n",
      " |  \n",
      " |  evaluate(self, input_fn, steps=None, hooks=None, checkpoint_path=None, name=None)\n",
      " |      Evaluates the model given evaluation data input_fn.\n",
      " |      \n",
      " |      For each step, calls `input_fn`, which returns one batch of data.\n",
      " |      Evaluates until:\n",
      " |      - `steps` batches are processed, or\n",
      " |      - `input_fn` raises an end-of-input exception (`OutOfRangeError` or\n",
      " |      `StopIteration`).\n",
      " |      \n",
      " |      Args:\n",
      " |        input_fn: Input function returning a tuple of:\n",
      " |            features - Dictionary of string feature name to `Tensor` or\n",
      " |              `SparseTensor`.\n",
      " |            labels - `Tensor` or dictionary of `Tensor` with labels.\n",
      " |        steps: Number of steps for which to evaluate model. If `None`, evaluates\n",
      " |          until `input_fn` raises an end-of-input exception.\n",
      " |        hooks: List of `SessionRunHook` subclass instances. Used for callbacks\n",
      " |          inside the evaluation call.\n",
      " |        checkpoint_path: Path of a specific checkpoint to evaluate. If `None`, the\n",
      " |          latest checkpoint in `model_dir` is used.\n",
      " |        name: Name of the evaluation if user needs to run multiple evaluations on\n",
      " |          different data sets, such as on training data vs test data. Metrics for\n",
      " |          different evaluations are saved in separate folders, and appear\n",
      " |          separately in tensorboard.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A dict containing the evaluation metrics specified in `model_fn` keyed by\n",
      " |        name, as well as an entry `global_step` which contains the value of the\n",
      " |        global step for which this evaluation was performed.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `steps <= 0`.\n",
      " |        ValueError: If no model has been trained, namely `model_dir`, or the\n",
      " |          given `checkpoint_path` is empty.\n",
      " |  \n",
      " |  export_savedmodel(self, export_dir_base, serving_input_receiver_fn, assets_extra=None, as_text=False, checkpoint_path=None)\n",
      " |      Exports inference graph as a SavedModel into given dir.\n",
      " |      \n",
      " |      For a detailed guide, see\n",
      " |      @{$saved_model#using_savedmodel_with_estimators$Using SavedModel with Estimators}.\n",
      " |      \n",
      " |      This method builds a new graph by first calling the\n",
      " |      serving_input_receiver_fn to obtain feature `Tensor`s, and then calling\n",
      " |      this `Estimator`'s model_fn to generate the model graph based on those\n",
      " |      features. It restores the given checkpoint (or, lacking that, the most\n",
      " |      recent checkpoint) into this graph in a fresh session.  Finally it creates\n",
      " |      a timestamped export directory below the given export_dir_base, and writes\n",
      " |      a `SavedModel` into it containing a single `MetaGraphDef` saved from this\n",
      " |      session.\n",
      " |      \n",
      " |      The exported `MetaGraphDef` will provide one `SignatureDef` for each\n",
      " |      element of the export_outputs dict returned from the model_fn, named using\n",
      " |      the same keys.  One of these keys is always\n",
      " |      signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY, indicating which\n",
      " |      signature will be served when a serving request does not specify one.\n",
      " |      For each signature, the outputs are provided by the corresponding\n",
      " |      `ExportOutput`s, and the inputs are always the input receivers provided by\n",
      " |      the serving_input_receiver_fn.\n",
      " |      \n",
      " |      Extra assets may be written into the SavedModel via the extra_assets\n",
      " |      argument.  This should be a dict, where each key gives a destination path\n",
      " |      (including the filename) relative to the assets.extra directory.  The\n",
      " |      corresponding value gives the full path of the source file to be copied.\n",
      " |      For example, the simple case of copying a single file without renaming it\n",
      " |      is specified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n",
      " |      \n",
      " |      Args:\n",
      " |        export_dir_base: A string containing a directory in which to create\n",
      " |          timestamped subdirectories containing exported SavedModels.\n",
      " |        serving_input_receiver_fn: A function that takes no argument and\n",
      " |          returns a `ServingInputReceiver`.\n",
      " |        assets_extra: A dict specifying how to populate the assets.extra directory\n",
      " |          within the exported SavedModel, or `None` if no extra assets are needed.\n",
      " |        as_text: whether to write the SavedModel proto in text format.\n",
      " |        checkpoint_path: The checkpoint path to export.  If `None` (the default),\n",
      " |          the most recent checkpoint found within the model directory is chosen.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The string path to the exported directory.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if no serving_input_receiver_fn is provided, no export_outputs\n",
      " |            are provided, or no checkpoint can be found.\n",
      " |  \n",
      " |  get_variable_names(self)\n",
      " |      Returns list of all variable names in this model.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of names.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the Estimator has not produced a checkpoint yet.\n",
      " |  \n",
      " |  get_variable_value(self, name)\n",
      " |      Returns value of the variable given by name.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: string or a list of string, name of the tensor.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Numpy array - value of the tensor.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the Estimator has not produced a checkpoint yet.\n",
      " |  \n",
      " |  latest_checkpoint(self)\n",
      " |      Finds the filename of latest saved checkpoint file in `model_dir`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The full path to the latest checkpoint or `None` if no checkpoint was\n",
      " |        found.\n",
      " |  \n",
      " |  predict(self, input_fn, predict_keys=None, hooks=None, checkpoint_path=None)\n",
      " |      Yields predictions for given features.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_fn: Input function returning features which is a dictionary of\n",
      " |          string feature name to `Tensor` or `SparseTensor`. If it returns a\n",
      " |          tuple, first item is extracted as features. Prediction continues until\n",
      " |          `input_fn` raises an end-of-input exception (`OutOfRangeError` or\n",
      " |          `StopIteration`).\n",
      " |        predict_keys: list of `str`, name of the keys to predict. It is used if\n",
      " |          the `EstimatorSpec.predictions` is a `dict`. If `predict_keys` is used\n",
      " |          then rest of the predictions will be filtered from the dictionary. If\n",
      " |          `None`, returns all.\n",
      " |        hooks: List of `SessionRunHook` subclass instances. Used for callbacks\n",
      " |          inside the prediction call.\n",
      " |        checkpoint_path: Path of a specific checkpoint to predict. If `None`, the\n",
      " |          latest checkpoint in `model_dir` is used.\n",
      " |      \n",
      " |      Yields:\n",
      " |        Evaluated values of `predictions` tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: Could not find a trained model in model_dir.\n",
      " |        ValueError: if batch length of predictions are not same.\n",
      " |        ValueError: If there is a conflict between `predict_keys` and\n",
      " |          `predictions`. For example if `predict_keys` is not `None` but\n",
      " |          `EstimatorSpec.predictions` is not a `dict`.\n",
      " |  \n",
      " |  train(self, input_fn, hooks=None, steps=None, max_steps=None, saving_listeners=None)\n",
      " |      Trains a model given training data input_fn.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_fn: Input function returning a tuple of:\n",
      " |            features - `Tensor` or dictionary of string feature name to `Tensor`.\n",
      " |            labels - `Tensor` or dictionary of `Tensor` with labels.\n",
      " |        hooks: List of `SessionRunHook` subclass instances. Used for callbacks\n",
      " |          inside the training loop.\n",
      " |        steps: Number of steps for which to train model. If `None`, train forever\n",
      " |          or train until input_fn generates the `OutOfRange` error or\n",
      " |          `StopIteration` exception. 'steps' works incrementally. If you call two\n",
      " |          times train(steps=10) then training occurs in total 20 steps. If\n",
      " |          `OutOfRange` or `StopIteration` occurs in the middle, training stops\n",
      " |          before 20 steps. If you don't want to have incremental behavior please\n",
      " |          set `max_steps` instead. If set, `max_steps` must be `None`.\n",
      " |        max_steps: Number of total steps for which to train model. If `None`,\n",
      " |          train forever or train until input_fn generates the `OutOfRange` error\n",
      " |          or `StopIteration` exception. If set, `steps` must be `None`. If\n",
      " |          `OutOfRange` or `StopIteration` occurs in the middle, training stops\n",
      " |          before `max_steps` steps.\n",
      " |          Two calls to `train(steps=100)` means 200 training\n",
      " |          iterations. On the other hand, two calls to `train(max_steps=100)` means\n",
      " |          that the second call will not do any iteration since first call did\n",
      " |          all 100 steps.\n",
      " |        saving_listeners: list of `CheckpointSaverListener` objects. Used for\n",
      " |          callbacks that run immediately before or after checkpoint savings.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `self`, for chaining.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If both `steps` and `max_steps` are not `None`.\n",
      " |        ValueError: If either `steps` or `max_steps` is <= 0.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.estimator.estimator.Estimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  config\n",
      " |  \n",
      " |  model_dir\n",
      " |  \n",
      " |  model_fn\n",
      " |      Returns the model_fn which is bound to self.params.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The model_fn with following signature:\n",
      " |          `def model_fn(features, labels, mode, config)`\n",
      " |  \n",
      " |  params\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.estimator.DNNClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /content/datalab/docs/log4/model.ckpt.\n",
      "INFO:tensorflow:loss = 2378.9788, step = 1\n",
      "INFO:tensorflow:global_step/sec: 5.70251\n",
      "INFO:tensorflow:loss = 168.8696, step = 101 (17.542 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.56725\n",
      "INFO:tensorflow:loss = 120.76572, step = 201 (17.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.48007\n",
      "INFO:tensorflow:loss = 38.455826, step = 301 (18.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.51459\n",
      "INFO:tensorflow:loss = 64.6398, step = 401 (18.135 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.55639\n",
      "INFO:tensorflow:loss = 61.548935, step = 501 (17.997 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.0861\n",
      "INFO:tensorflow:loss = 58.47879, step = 601 (19.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.39518\n",
      "INFO:tensorflow:loss = 63.47788, step = 701 (18.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.5193\n",
      "INFO:tensorflow:loss = 34.08306, step = 801 (18.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.44737\n",
      "INFO:tensorflow:loss = 11.313051, step = 901 (18.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.17291\n",
      "INFO:tensorflow:loss = 23.469917, step = 1001 (19.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.47278\n",
      "INFO:tensorflow:loss = 16.85314, step = 1101 (18.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.30652\n",
      "INFO:tensorflow:loss = 10.0748205, step = 1201 (18.845 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.56623\n",
      "INFO:tensorflow:loss = 4.531226, step = 1301 (17.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.60919\n",
      "INFO:tensorflow:loss = 3.7647142, step = 1401 (17.828 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.64696\n",
      "INFO:tensorflow:loss = 4.0642877, step = 1501 (17.710 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.47159\n",
      "INFO:tensorflow:loss = 3.1144776, step = 1601 (18.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.50441\n",
      "INFO:tensorflow:loss = 2.1927857, step = 1701 (18.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.59645\n",
      "INFO:tensorflow:loss = 1.8750973, step = 1801 (17.869 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.60256\n",
      "INFO:tensorflow:loss = 2.4677715, step = 1901 (17.848 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /content/datalab/docs/log4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.3392146.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7f49434b9550>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn=train_input_fn, steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 6043. Click <a href=\"/_proxy/53156/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6043"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb.start('/content/datalab/docs/log4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
